1.hive分为管理表和外部表：区别在于load和drop表时，前者会加载数据到数据仓库目录，删除表时会将数据也删除，
而外部表可以在创建表的时候指定数据的路径，在删除外部表的时候，只删除表的元数据信息，而不会删除数据本身
根据经验，如果所有处理都由hive来完成，应使用管理表，如果要用hive和其他工具来处理同一个数据集，应该使用
外部表，也可以使用
insert overwrite directory把数据导出到hadoop文件系统中

2.分区和分桶
分区：（针对的是数据的存储路径）
创建二级分区表
create table dept_partition2(
               deptno int, dname string, loc string
               )
               partitioned by (month string, day string)
               row format delimited fields terminated by '\t';
加载数据
load data local inpath '/opt/module/datas/dept.txt' into table default.dept_partition2
partition(month='201709', day='13');
查询分区数据
 select * from dept_partition2 where month='201709' and day='13';

 把数据直接上传到分区目录上，让分区表和数据产生关联的两种方式
 （1）方式一：上传数据后修复
 上传数据
 dfs -put /opt/module/datas/dept.txt  /user/hive/warehouse/dept_partition2/month=201709/day=12;
执行修复命令：msck repair table dept_partition2
（2）方式二：上传数据后添加分区
上传数据：同上
执行添加分区： alter table dept_partition2 add partition(month='201709', day='11');
（3）方式三：上传数据后load数据到分区
创建目录再上传数据
多分区联合查询：select * from a where month='201709' union select * from a where month='201708';
添加分区： alter table dept_partition add partition(month='201706') partition(month='201705');
（空格隔开）
删除分区：alter table dept_partition drop partition (month='201705'),partition (month='201706');
（逗号隔开）

动态分区
参数设置
1.开启动态分区功能：hive.exec.dynamic.partition=true
2.设置为非严格模式：hive.exec.dynamic.partition.mode=nonstrict
3.在所有执行MR的节点上，最大一共可以创建多少个动态分区：hive.exec.max.dynamic.partitions=1000
4.在每个执行MR的节点上，最大可以创建多少个动态分区：hive.exec.max.dynamic.partitions.pernode=100
5.整个MR Job中，最大可以创建多少个HDFS文件：hive.exec.max.created.files=100000
创建目标分区表
create table ori_partitioned_target(id bigint, time bigint, uid string, keyword string, url_rank int,
click_num int, click_url string) PARTITIONED BY (p_time STRING) row format delimited fields terminated by '\t';
插入数据：insert overwrite table ori_partitioned_target partition (p_time)
     select id, time, uid, keyword, url_rank, click_num, click_url, p_time from ori_partitioned;



分桶及抽样查询：（针对的是数据文件）
开启分桶：set hive.enforce.bucketing=true;
分桶抽样查询
hive (default)> select * from stu_buck tablesample(bucket 1 out of 4 on id);
注：tablesample是抽样语句，语法：TABLESAMPLE(BUCKET x OUT OF y) 。
y必须是table总bucket数的倍数或者因子。hive根据y的大小，决定抽样的比例。例如，table总共分了4份，当y=2
时，抽取(4/2=)2个bucket的数据，当y=8时，抽取(4/8=)1/2个bucket的数据。
数据块抽样（基于行数的，按照输入路径下的数据块百分比进行的抽样）
select * from stu tablesample(0.1 percent) ;



3.查看内置函数使用方法
DESCRIBE FUNCTION EXTENDED 函数名;
另一种函数是聚合函数。所有的聚合函数、用户自定义函数和内置函数，都统称为用户自定义聚合函数（UDAF）。聚合
方法通常和GROUP BY语句组合使用。
和其他函数类别一样，所有的表生成函数，包括用户自定义的和内置的，都统称为用户自定义表生成函数（UDTF）。
hive> SELECT explode(array(1,2,3)) AS element FROM src;
1
2
3

hive常见操作
添加列： alter table dept_partition add columns(deptdesc string);
更新列：alter table dept_partition change column deptdesc desc int;
替换列：alter table dept_partition replace columns(deptno string, dname string, loc string);
单表查询插入： insert overwrite table student partition(month='201708')
                    select id, name from student where month='201709';
多表查询插入：

将查询结果导出到本地linux：
insert overwrite local（不加local字段，结果导出到hdfs） directory '/opt/module/datas/export/student'
                              ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'（格式化）
                            select * from student;
hadoop命令导出：
hive (default)> dfs -get /user/hive/warehouse/student/month=201709/000000_0  /opt/module/datas
/export/student3.txt;
hive shell导出：bin/hive -e 'select * from default.student;' > /opt/module/datas/export/
student4.txt;
清空表： truncate table student;（只能清空管理表，不能删除外部表中的数据）

查询工资在500到1000的员工信息
 select * from emp where sal between 500 and 1000;
查询工资是1500和5000的员工信息
 select * from emp where sal IN (1500, 5000);
 like和rlike
 （1）查找以2开头薪水的员工信息
 	hive (default)> select * from emp where sal LIKE '2%';
 	（2）查找第二个数值为2的薪水的员工信息
 hive (default)> select * from emp where sal LIKE '_2%';
 	（3）查找薪水中含有2的员工信息
 hive (default)> select * from emp where sal RLIKE '[2]';
查询除了20部门和30部门以外的员工信息
select * from emp where deptno not IN(30, 20);
group by语句
（1）计算emp表每个部门的平均工资
hive (default)> select t.deptno, avg(t.sal) avg_sal from emp t group by t.deptno;
	（2）计算emp每个部门中每个岗位的最高薪水
hive (default)> select t.deptno, t.job, max(t.sal) max_sal from emp t group by t.deptno, t.job;
hiving 语句
having与where不同点
（1）where针对表中的列发挥作用，查询数据；having针对查询结果中的列发挥作用，筛选数据。
（2）where后面不能写分组函数，而having后面可以使用分组函数。
（3）having只用于group by分组统计语句。
求每个部门的平均薪水大于2000的部门
hive (default)> select deptno, avg(sal) avg_sal from emp group by deptno having avg_sal > 2000;
join语句
Hive支持通常的SQL JOIN语句，但是只支持等值连接，不支持非等值连接。
内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。
左外连接：JOIN操作符左边表中符合WHERE子句的所有记录将会被返回。
右外连接：JOIN操作符右边表中符合WHERE子句的所有记录将会被返回。
满外连接：将会返回所有表中符合WHERE语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么
就使用NULL值替代。
多表连接：连接 n个表，至少需要n-1个连接条件。例如：连接三个表，至少需要两个连接条件。
SELECT e.ename, d.deptno, l. loc_name
FROM   emp e
JOIN   dept d
ON     d.deptno = e.deptno
JOIN   location l
ON     d.loc = l.loc;

排序：
全局排序（Order By，一个mr）：如果在严格模式下，order by需要指定limit数据条数，不然数据量巨大的情况下会
造成崩溃无输出结果。涉及属性：set hive.mapred.mode=nonstrict/strict
每个MapReduce内部排序（Sort By）：每个MapReduce内部进行排序，对全局结果集来说不是排序。
分区排序（Distribute By）：类似MR中partition，进行分区，结合sort by使用。
注意，Hive要求DISTRIBUTE BY语句要写在SORT BY语句之前。
例：不同的人（personId）分为不同的组，每组按照money排序。
select * from company_info distribute by personId sort by personId, money desc;
对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。
cluster by 等价于distribute by...sort by...
hive (default)> select * from emp cluster by deptno;
hive (default)> select * from emp distribute by deptno sort by deptno;

常用语句
1.嵌套select语句
hive> FROM (
　　>　　SELECT upper(name), salary, deductions["Federal Taxes"] as fed_taxes,
　　>　　round(salary * (1 - deductions["Federal Taxes"])) as salary_ minus_fed_taxes
　　>　FROM employees
　　>　)　　e
　　> SELECT e.name, e.salary_minus_fed_taxes
　　> WHERE e.salary_minus_fed_taxes > 70000;
JOHN DOE　100000.0　0.2　80000
从这个嵌套查询语句中可以看到，我们将前面的结果集起了个别名，称之为e，在这个语句外面嵌套查询了name和
salary_minus_fed_taxes
两个字段，同时约束后者的值要大于70，000。

2.CASE … WHEN … THEN 句式
CASE … WHEN … THEN语句和if条件语句类似，用于处理单个列的查询结果。
hive> SELECT name, salary,
　　>　　CASE
　　>　　　 WHEN salary < 50000.0 THEN 'low'
　　>　　　 WHEN salary >= 50000.0 AND salary < 70000.0 THEN 'middle'
　　>　　　 WHEN salary >= 70000.0 AND salary < 100000.0 THEN 'high'
　　>　　　 ELSE 'very high'
　　>　　END AS bracket FROM employees;
John Doe　　　　 100000.0 very high
Mary Smith　　　  80000.0 high
Todd Jones　　　  70000.0 high
Bill King　　　　 60000.0 middle
Boss Man　　　　　200000.0 very high
Fred Finance　　　150000.0 very high
Stacy Accountant　 60000.0 middle

3.ctas句式
create table aaa as select ...建立存储查询结果的表，只能是管理表

4.insert into table select ...将查询数据插入到目标表

5.where 语句
hive> SELECT name, salary, deductions["Federal Taxes"],
　　>　　salary * (1 - deductions["Federal Taxes"])
　　> FROM employees
　　> WHERE round(salary * (1 - deductions["Federal Taxes"])) > 70000;
John Doe　　　100000.0　0.2　 80000.0
这个查询语句有点难看，因为第2行的那个复杂的表达式和WHERE后面的表达式是一样的，可以用下面的语句。注
意where语句中不能使用列别名
hive> SELECT e.* FROM
　　> (SELECT name, salary, deductions["Federal Taxes"] as ded,
　　>　　salary * (1 - deductions["Federal Taxes"]) as salary_minus_fed_taxes
　　>　 FROM employees) e
　　> WHERE round(e.salary_minus_fed_taxes) > 70000;
John Doe　　　　100000.0　　0.2　　　80000.0



hive调优

1.使用explain，EXPLAIN EXTENDED语句
查看抽象语法树，用于针对执行时间长的复杂sql语句进行分析
把hive.fetch.task.conversion设置成more
2.限制调整
LIMIT语句是大家经常使用到的，经常使用CLI的用户都会使用到。不过，在很多情况下LIMIT语句还是需要执行
整个查询语句，然后再返回部分
结果的。因为这种情况通常是浪费的，所以应该尽可能地避免出现这种情况。Hive有一个配置属性可以开启，当
使用LIMTI语句时，其可以对源
数据进行抽样：
<property>
　<name>hive.limit.optimize.enable</name>
　<value>true</value>
　<description>Whether to enable to optimization to
　　try a smaller subset of data for simple LIMIT first.</description>
</property>
一旦属性hive.limit.optimize.enable的值设置为true，那么还会有两个参数可以控制这个操作，也就是
hive.limit.row.max.size和hive.limit.optimize.limit.file：
<property>
　<name>hive.limit.row.max.size</name>
　<value>100000</value>
　<description>When trying a smaller subset of data for simple LIMIT,
　　how much size we need to guarantee each row to have at least.
　</description>
</property>

<property>
　<name>hive.limit.optimize.limit.file</name>
　<value>10</value>
　<description>When trying a smaller subset of data for simple LIMIT,
　　maximum number of files we can sample.</description>
</property>
这个功能的一个缺点就是，有可能输入中有用的数据永远不会被处理到，例如，像任意的一个需要reduce步骤的查
询，JOIN和GROUP BY操作，以及聚合函数的大多数调用，等等，将会产生很不同的结果。也许这个差异在很多情况
下是可以接受的，但是重要的是要理解。

3.join优化
将key相对分散，并且数据量小的表放在join的左边，这样可以有效减少内存溢出错误发生的几率；再进一步，可
以使用Group让小的维度表（1000条以下的记录条数）先进内存。在map端完成reduce。
实际测试发现：新版的hive已经对小表JOIN大表和大表JOIN小表进行了优化。小表放在左边和右边已经没有明显区别。
大表Join大表：
1）空KEY过滤
有时join超时是因为某些key对应的数据太多，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。
此时我们应该仔细分析这些异常的key，很多情况下，这些key对应的数据是异常数据，我们需要在SQL语句中进行过滤。
不过滤空key：insert overwrite table jointable
select n.* from nullidtable n left join ori o on n.id = o.id;
过滤空key： insert overwrite table jointable
       select n.* from (select * from nullidtable where id is not null ) n  left join ori o on n.id = o.id;
2）空key转换
有时虽然某个key为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join的结果中，此时我们可以表a中key为
空的字段赋一个随机的值，使得数据随机均匀地分不到不同的reducer上。

mapjoin
如果不指定MapJoin或者不符合MapJoin的条件，那么Hive解析器会将Join操作转换成Common Join，即：在Reduce阶段完成join。
容易发生数据倾斜。可以用MapJoin把小表全部加载到内存在map端进行join，避免reducer处理。
1）开启MapJoin参数设置：
（1）设置自动选择Mapjoin
set hive.auto.convert.join = true; 默认为true
（2）大表小表的阈值设置（默认25M一下认为是小表）：
set hive.mapjoin.smalltable.filesize=25000000;

4.本地模式
大多数的Hadoop Job是需要Hadoop提供的完整的可扩展性来处理大数据集的。不过，有时Hive的输入数据量是非
常小的。在这种情况下，为查询触发执行任务的时间消耗可能会比实际job的执行时间要多得多。对于大多数这种
情况，Hive可以通过本地模式在单台机器上（或某些时候在单个进程中）处理所有的任务。对于小数据集，执行
时间可以明显被缩短。
 set mapred.job.tracker=local;
<property>
　<name>hive.exec.mode.local.auto</name>
　<value>true</value>
　<description>
　　Let hive determine whether to run in local mode automatically
　</description>
</property>

5.并行执行
Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段，或者Hive执行过程中可能
需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，
也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么job可能就越快完成。
通过设置参数hive.exec.parallel值为true，就可以开启并发执行。不过，在共享集群中，需要注意下，如果job中并行执行的阶段增多，那
么集群利用率就会增加：
<property>
　<name>hive.exec.parallel</name>
　<value>true</value>
　<description>Whether to execute jobs in parallel</description>
</property>

6.严格模式
Hive提供了一个严格模式，可以防止用户执行那些可能产生意想不到的不好的影响的查询。
通过设置属性hive.mapred.mode值为strict可以禁止3种类型的查询。
其一，对于分区表，除非WHEHRE语句中含有分区字段过滤条件来限制数据范围，否则不允许执行。换句话说，就是用户不允许扫描所有分区。
进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资
源来处理这个表
其二，对于使用了ORDER BY 语句的查询，要求必须使用LIMIT语句。因为ORDER BY为了执行排序过程会将所有的结果数据分发到同一个
reducer中进行处理，强制要求用户增加这个LIMIT语句可以防止reducer额外执行很长一段时间
其三，也就是最后一种情况，就是限制笛卡尔积的查询。对关系型数据库非常了解的用户可能期望在执行JOIN查询的时候不使用ON语句而是使
用WHERE语句，这样关系型数据库的执行优化器就可以高效地将WHERE语句转化成那个ON语句。不幸的是，Hive并不会执行这种优化，因此，
如果表足够大，那么这个查询就会出现不可控的情况

7.调整mapper和reduce个数
Hive通过将查询划分成一个或者多个MapReduce任务达到并行的目的。每个任务都可能具有多个mapper和reducer任务，其中至少有一些是可以
并行执行的。确定最佳的mapper个数和reducer个数取决于多个变量，例如输入的数据量大小以及对这些数据执行的操作类型等。
保持平衡性是有必要的。如果有太多的mapper或reducer任务，就会导致启动阶段、调度和运行job过程中产生过多的开销；而如果设置的数量
太少，那么就可能没有充分利用好集群内在的并行性
一个快速的进行验证的方式就是将reducer个数设置为固定的值，而无需Hive来计算得到这个值。如果用户还记得的话，Hive的默认reducer
个数应该是3。可以通过设置属性mapred.reduce.tasks的值为不同的值来确定是使用较多还是较少的reducer来缩短执行时间。需要记住，
受外部因素影响，像这样的标杆值十分复杂，例如其他用户并发执行job的情况。Hadoop需要消耗好几秒时间来启动和调度map和reduce任
务（task）。在进行性能测试的时候，要考虑到这些影响因子，特别是job比较小的时候。
当在共享集群上处理大任务时，为了控制资源利用情况，属性hive.exec.reducers.max显得非常重要。一个Hadoop集群可以提供的map和
reduce资源个数（也称为“插槽”），是固定的。某个大job可能就会消耗完所有的插槽，从而导致其他job无法执行。通过设置属性
hive.exec.reducers.max可以阻止某个查询消耗太多的reducer资源。有必要将这个属性配置到$HIVE_HOME/conf/hive-site.xml文件中。
对这个属性值大小的一个建议的计算公式如下：
（集群总Reduce槽位个数*1.5）/(执行中的查询的平均个数)
 1.5倍数是一个经验系数，用于防止未充分利用集群的情况。

 8.jvm重用
 JVM重用是Hadoop调优参数的内容，其对Hive的性能具有非常大的影响，特别是对于很难避免小文件的场景或task特别多的场景，这类场
 景大多数执行时间都很短。
 Hadoop的默认配置通常是使用派生JVM来执行map和reduce任务的。这时JVM的启动过程可能会造成相当大的开销，尤其是执行的job包含有
 成百上千个task任务的情况。JVM重用可以使得JVM实例在同一个job中重新使用N次。N的值可以在Hadoop的mapred-site.xml文件
 （位于$HADOOP_HOME/conf目录下）中进行设置：
 <property>
 　<name>mapred.job.reuse.jvm.num.tasks</name>
 　<value>10</value>
 　<description>How many tasks to run per jvm. If set to -1, there is no limit.
 　</description>
 </property>
 这个功能的一个缺点是，开启JVM重用将会一直占用使用到的task插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡的”的job中
 有某几个reduce task执行的时间要比其他reduce task消耗的时间多得多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，
 直到所有的task都结束了才会释放。

 9.索引

 10.动态分区调整
 配置是来控制DataNode上一次可以打开的文件的个数。这个参数必须设置在DataNode的$HADOOP_HOME/conf/hdfs-site.xml配置文件中。
 在Hadoop v0.20.2版本中，这个属性的默认值是256，太小了。这个值会影响到最大的线程数和资源数，因此，也并不推荐将这个属性值设置
 为一个极大值。同时需要注意的是，在Hadoop v0.20.2版本中，更改这个属性值需要重启DataNode才能够生效：
 <property>
 　<name>dfs.datanode.max.xcievers</name>
 　<value>8192</value>
 </property>

11.推测执行
推测执行是Hadoop中的一个功能，其可以触发执行一些重复的任务（task）。尽管这样会因对重复的数据进行计算而导致消耗更多的
计算资源，不过这个功能的目标是通过加快获取单个task的结果以及进行侦测将执行慢的TaskTracker加入到黑名单的方式来提高整体
的任务执行效率
Hadoop的推测执行功能由$HADOOP_HOME/conf/mapred-site.xml文件中的如下2个配置项控制着：
<property>
　<name>mapred.map.tasks.speculative.execution</name>
　<value>true</value>
　<description>If true, then multiple instances of some map tasks
　may be executed in parallel.</description>
</property>

<property>
　<name>mapred.reduce.tasks.speculative.execution</name>
　<value>true</value>
　<description>If true, then multiple instances of some reduce tasks
　may be executed in parallel.</description>
</property>
不过，Hive本身也提供了配置项来控制reduce-side的推测执行：
<property>
　<name>hive.mapred.reduce.tasks.speculative.execution</name>
　<value>true</value>
　<description>Whether speculative execution for
　reducers should be turned on. </description>
</property>
关于调优这些推测执行变量，还很难给一个具体的建议。如果用户对于运行时的偏差非常敏感的话，那么可以将这些功能关闭掉。如果用户
因为输入数据量很大而需要执行长时间的map或者reduce task的话，那么启动推测执行造成的浪费是非常巨大的。

12.单个MapReduce中多个GROUP BY
另一个特别的优化试图将查询中的多个GROUP BY操作组装到单个MapReduce任务中。如果想启动这个优化，那么需要一组常用的GROUP BY键：
<property>
　<name>hive.multigroupby.singlemr</name>
　<value>false</value>
　<description>Whether to optimize multi group by query to generate single M/R
　job plan. If the multi group by query has common group by keys, it will be
　optimized to generate single M/R job.</description>
</property>

13.虚拟列
Hive提供了2种虚拟列：一种用于将要进行划分的输入文件名，另一种用于文件中的块内偏移量。当Hive产生了非预期的或null的返回结果时，
可以通过这些虚拟列诊断查询。通过查询这些“字段”，用户可以查看到哪个文件甚至哪行数据导致出现问题
hive> set hive.exec.rowoffset=true;

hive> SELECT INPUT__FILE__NAME, BLOCK__OFFSET__INSIDE__FILE, line
　　> FROM hive_text WHERE line LIKE '%hive%' LIMIT 2;
har://file/user/hive/warehouse/hive_text/folder=docs/
data.har/user/hive/warehouse/hive_text/folder=docs/README.txt 2243
　　　　　　http://hive.apache.org/

har://file/user/hive/warehouse/hive_text/folder=docs/
data.har/user/hive/warehouse/hive_text/folder=docs/README.txt 3646
- Hive 0.8.0 ignores the hive-default.xml file, though we continue

14.压缩
压缩通常都会节约可观的磁盘空间，例如，基于文本的文件可以压缩40%甚至更高比例。压缩同样可以增加吞吐量和性能。这看上去似乎并不合
常理，因为压缩和解压缩会增加额外的CPU开销，不过，通过减少载入内存的数据量而提高I/O吞吐量会更加提高网络传输性能。
Hadoop的job通常是I/O密集型而不是CPU密集型的。如果是这样的话，压缩可以提高性能。不过，如果用户的job是CPU密集型的话，那么使用压
缩可能会降低执行性能。确定是否进行压缩的唯一方法就是尝试不同的选择，并测量对比执行结果。
每一个压缩方案都在压缩/解压缩速度和压缩率间进行权衡。BZip2压缩率最高，但是同时需要消耗最多的CPU开销。GZip是压缩率和压缩/解
压缩速度上的下一个选择。因此，如果磁盘空间利用率和I/O开销都需要考虑的话，那么这2种压缩方案都是有吸引力的。
LZO和Snappy压缩率相比前面的2种要小但是压缩/解压缩速度要快，特别是解压缩过程。如果相对于磁盘空间和I/O开销，频繁读取数据所需
的解压缩速度更重要的话，那么它们将是不错的选择。
对中间结果数据压缩
对中间数据进行压缩可以减少job中map和reduce task间的数据传输量。对于中间数据压缩，选择一个低CPU开销的编/解码器要比选择一个
压缩率高的编/解码器要重要得多。属性hive.exec.compress. intermediate的默认值是false，如果要开启中间压缩，就需要将这个属性
值修改为默认值为true
最终数据结果压缩
当Hive将输出写入到表中时，输出内容同样可以进行压缩。属性hive.exec. compress.output控制着这个功能。用户可能需要保持默认配
置文件中的默认值false，这样默认的输出就是非压缩的纯文本文件了。用户可以通过在查询语句或执行脚本中设置这个值为true，来开启
输出结果压缩功能：
<property>
　<name>hive.exec.compress.output</name>
    <value>false</value>
　   <description> This controls whether the final outputs of a query
　   (to a local/hdfs file or a Hive table) is compressed. The compression
　   codec and other options are determined from hadoop config variables
　   mapred.output.compress* </description>
</property>

